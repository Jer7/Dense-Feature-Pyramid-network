{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4803c441ae64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "import os\n",
    "import os.path as osp\n",
    "from networks.CE2P import Res_Deeplab\n",
    "from dataset.datasets import LIPDataSet\n",
    "import torchvision.transforms as transforms\n",
    "import timeit\n",
    "from tensorboardX import SummaryWriter\n",
    "from utils.utils import decode_parsing, inv_preprocess\n",
    "from utils.criterion import CriterionAll\n",
    "from utils.encoding import DataParallelModel, DataParallelCriterion \n",
    "from utils.miou import compute_mean_ioU\n",
    "from evaluate import valid\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# CS_PATH='./dataset/LIP/TrainVal'\n",
    "# LR=1e-3\n",
    "# WD=5e-4\n",
    "# BS=24\n",
    "# GPU_IDS=0,1,2,3\n",
    "# RESTORE_FROM='./dataset/LIP/resnet101-imagenet.pth'\n",
    "# INPUT_SIZE='384,384'  \n",
    "# SNAPSHOT_DIR='./snapshots'\n",
    "# DATASET='train'\n",
    "# NUM_CLASSES=20 \n",
    "# EPOCHS=150 \n",
    "\n",
    "BATCH_SIZE = 8\n",
    "DATA_DIRECTORY = './dataset/LIP/TrainVal'\n",
    "DATA_LIST_PATH = './dataset/list/cityscapes/train.lst'\n",
    "IGNORE_LABEL = 255\n",
    "INPUT_SIZE = '384,384' \n",
    "LEARNING_RATE = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "NUM_CLASSES = 20\n",
    "POWER = 0.9\n",
    "RANDOM_SEED = 1234\n",
    "RESTORE_FROM = './dataset/LIP/resnet101-imagenet.pth'\n",
    "SAVE_NUM_IMAGES = 2\n",
    "SAVE_PRED_EVERY = 10000\n",
    "SNAPSHOT_DIR = './snapshots/'\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# BATCH_SIZE = 8\n",
    "# DATA_DIRECTORY = 'cityscapes'\n",
    "# DATA_LIST_PATH = './dataset/list/cityscapes/train.lst'\n",
    "# IGNORE_LABEL = 255\n",
    "# INPUT_SIZE = '769,769'\n",
    "# LEARNING_RATE = 1e-2\n",
    "# MOMENTUM = 0.9\n",
    "# NUM_CLASSES = 20\n",
    "# POWER = 0.9\n",
    "# RANDOM_SEED = 1234\n",
    "# RESTORE_FROM = './dataset/MS_DeepLab_resnet_pretrained_init.pth'\n",
    "# SAVE_NUM_IMAGES = 2\n",
    "# SAVE_PRED_EVERY = 10000\n",
    "# SNAPSHOT_DIR = './snapshots/'\n",
    "# WEIGHT_DECAY = 0.0005\n",
    " \n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def get_arguments():\n",
    "    \"\"\"Parse all the arguments provided from the CLI.\n",
    "\n",
    "    Returns:\n",
    "      A list of parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"CE2P Network\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE,\n",
    "                        help=\"Number of images sent to the network in one step.\")\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=DATA_DIRECTORY,\n",
    "                        help=\"Path to the directory containing the dataset.\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='train', choices=['train', 'val', 'trainval', 'test'],\n",
    "                        help=\"Path to the file listing the images in the dataset.\")\n",
    "    parser.add_argument(\"--ignore-label\", type=int, default=IGNORE_LABEL,\n",
    "                        help=\"The index of the label to ignore during the training.\")\n",
    "    parser.add_argument(\"--input-size\", type=str, default=INPUT_SIZE,\n",
    "                        help=\"Comma-separated string with height and width of images.\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=LEARNING_RATE,\n",
    "                        help=\"Base learning rate for training with polynomial decay.\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=MOMENTUM,\n",
    "                        help=\"Momentum component of the optimiser.\")\n",
    "    parser.add_argument(\"--num-classes\", type=int, default=NUM_CLASSES,\n",
    "                        help=\"Number of classes to predict (including background).\") \n",
    "    parser.add_argument(\"--start-iters\", type=int, default=0,\n",
    "                        help=\"Number of classes to predict (including background).\") \n",
    "    parser.add_argument(\"--power\", type=float, default=POWER,\n",
    "                        help=\"Decay parameter to compute the learning rate.\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=WEIGHT_DECAY,\n",
    "                        help=\"Regularisation parameter for L2-loss.\")\n",
    "    parser.add_argument(\"--random-mirror\", action=\"store_true\",\n",
    "                        help=\"Whether to randomly mirror the inputs during the training.\")\n",
    "    parser.add_argument(\"--random-scale\", action=\"store_true\",\n",
    "                        help=\"Whether to randomly scale the inputs during the training.\")\n",
    "    parser.add_argument(\"--random-seed\", type=int, default=RANDOM_SEED,\n",
    "                        help=\"Random seed to have reproducible results.\")\n",
    "    parser.add_argument(\"--restore-from\", type=str, default=RESTORE_FROM,\n",
    "                        help=\"Where restore model parameters from.\")\n",
    "    parser.add_argument(\"--save-num-images\", type=int, default=SAVE_NUM_IMAGES,\n",
    "                        help=\"How many images to save.\")\n",
    "    parser.add_argument(\"--snapshot-dir\", type=str, default=SNAPSHOT_DIR,\n",
    "                        help=\"Where to save snapshots of the model.\")\n",
    "    parser.add_argument(\"--gpu\", type=str, default='None',\n",
    "                        help=\"choose gpu device.\")\n",
    "    parser.add_argument(\"--start-epoch\", type=int, default=0,\n",
    "                        help=\"choose the number of recurrence.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=150,\n",
    "                        help=\"choose the number of recurrence.\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "args = get_arguments()\n",
    "\n",
    "\n",
    "def lr_poly(base_lr, iter, max_iter, power):\n",
    "    return base_lr * ((1 - float(iter) / max_iter) ** (power))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, i_iter, total_iters):\n",
    "    \"\"\"Sets the learning rate to the initial LR divided by 5 at 60th, 120th and 160th epochs\"\"\"\n",
    "    lr = lr_poly(args.learning_rate, i_iter, total_iters, args.power)\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def adjust_learning_rate_pose(optimizer, epoch):\n",
    "    decay = 0\n",
    "    if epoch + 1 >= 230:\n",
    "        decay = 0.05\n",
    "    elif epoch + 1 >= 200:\n",
    "        decay = 0.1\n",
    "    elif epoch + 1 >= 120:\n",
    "        decay = 0.25\n",
    "    elif epoch + 1 >= 90:\n",
    "        decay = 0.5\n",
    "    else:\n",
    "        decay = 1\n",
    "\n",
    "    lr = args.learning_rate * decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def set_bn_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.eval()\n",
    "\n",
    "\n",
    "def set_bn_momentum(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1 or classname.find('InPlaceABN') != -1:\n",
    "        m.momentum = 0.0003\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Create the model and start the training.\"\"\"\n",
    "\n",
    "    if not os.path.exists(args.snapshot_dir):\n",
    "        os.makedirs(args.snapshot_dir)\n",
    "\n",
    "    writer = SummaryWriter(args.snapshot_dir)\n",
    "    gpus = [int(i) for i in args.gpu.split(',')]\n",
    "    if not args.gpu == 'None':\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "    h, w = map(int, args.input_size.split(','))\n",
    "    input_size = [h, w]\n",
    "\n",
    "    cudnn.enabled = True\n",
    "    # cudnn related setting\n",
    "    cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.enabled = True\n",
    " \n",
    "\n",
    "    deeplab = Res_Deeplab(num_classes=args.num_classes)\n",
    "\n",
    "    # dump_input = torch.rand((args.batch_size, 3, input_size[0], input_size[1]))\n",
    "    # writer.add_graph(deeplab.cuda(), dump_input.cuda(), verbose=False)\n",
    "\n",
    "    saved_state_dict = torch.load(args.restore_from)\n",
    "    new_params = deeplab.state_dict().copy()\n",
    "    for i in saved_state_dict:\n",
    "        i_parts = i.split('.')\n",
    "        # print(i_parts)\n",
    "        if not i_parts[0] == 'fc':\n",
    "            new_params['.'.join(i_parts[0:])] = saved_state_dict[i]\n",
    "\n",
    "    deeplab.load_state_dict(new_params)\n",
    "   \n",
    "    model = DataParallelModel(deeplab)\n",
    "    model.cuda()\n",
    "\n",
    "    criterion = CriterionAll()\n",
    "    criterion = DataParallelCriterion(criterion)\n",
    "    criterion.cuda()\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    trainloader = data.DataLoader(LIPDataSet(args.data_dir, args.dataset, crop_size=input_size, transform=transform),\n",
    "                                  batch_size=args.batch_size * len(gpus), shuffle=True, num_workers=2,\n",
    "                                  pin_memory=True)\n",
    "    #lip_dataset = LIPDataSet(args.data_dir, 'val', crop_size=input_size, transform=transform)\n",
    "    #num_samples = len(lip_dataset)\n",
    "    \n",
    "    #valloader = data.DataLoader(lip_dataset, batch_size=args.batch_size * len(gpus),\n",
    "    #                             shuffle=False, pin_memory=True)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    total_iters = args.epochs * len(trainloader)\n",
    "    print('Total Iters:',total_iters)\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        model.train()\n",
    "        for i_iter, batch in enumerate(trainloader):\n",
    "            i_iter += len(trainloader) * epoch\n",
    "            lr = adjust_learning_rate(optimizer, i_iter, total_iters)\n",
    "            print('i_iter:',i_iter, ' and lr:',lr )\n",
    "            images, labels, edges, _ = batch\n",
    "            labels = labels.long().cuda(non_blocking=True)\n",
    "            edges = edges.long().cuda(non_blocking=True)\n",
    "\n",
    "            preds = model(images)\n",
    "\n",
    "            loss = criterion(preds, [labels, edges])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i_iter % 100 == 0:\n",
    "                writer.add_scalar('learning_rate', lr, i_iter)\n",
    "                writer.add_scalar('loss', loss.data.cpu().numpy(), i_iter)\n",
    "\n",
    "            if i_iter % 500 == 0:\n",
    "\n",
    "                images_inv = inv_preprocess(images, args.save_num_images)\n",
    "                labels_colors = decode_parsing(labels, args.save_num_images, args.num_classes, is_pred=False)\n",
    "                edges_colors = decode_parsing(edges, args.save_num_images, 2, is_pred=False)\n",
    "\n",
    "                if isinstance(preds, list):\n",
    "                    preds = preds[0]\n",
    "                preds_colors = decode_parsing(preds[0][-1], args.save_num_images, args.num_classes, is_pred=True)\n",
    "                pred_edges = decode_parsing(preds[1][-1], args.save_num_images, 2, is_pred=True)\n",
    "\n",
    "                img = vutils.make_grid(images_inv, normalize=False, scale_each=True)\n",
    "                lab = vutils.make_grid(labels_colors, normalize=False, scale_each=True)\n",
    "                pred = vutils.make_grid(preds_colors, normalize=False, scale_each=True)\n",
    "                edge = vutils.make_grid(edges_colors, normalize=False, scale_each=True)\n",
    "                pred_edge = vutils.make_grid(pred_edges, normalize=False, scale_each=True)\n",
    "\n",
    "                writer.add_image('Images/', img, i_iter)\n",
    "                writer.add_image('Labels/', lab, i_iter)\n",
    "                writer.add_image('Preds/', pred, i_iter)\n",
    "                writer.add_image('Edges/', edge, i_iter)\n",
    "                writer.add_image('PredEdges/', pred_edge, i_iter)\n",
    "\n",
    "            print('iter = {} of {} completed, loss = {}'.format(i_iter, total_iters, loss.data.cpu().numpy()))\n",
    "\n",
    "        torch.save(model.state_dict(), osp.join(args.snapshot_dir, 'LIP_epoch_' + str(epoch) + '.pth'))\n",
    "\n",
    "        #parsing_preds, scales, centers = valid(model, valloader, input_size,  num_samples, len(gpus))\n",
    "\n",
    "        #mIoU = compute_mean_ioU(parsing_preds, scales, centers, args.num_classes, args.data_dir, input_size)\n",
    "\n",
    "        #print(mIoU)\n",
    "        #writer.add_scalars('mIoU', mIoU, epoch)\n",
    "\n",
    "    end = timeit.default_timer()\n",
    "    print(end - start, 'seconds')\n",
    " \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
